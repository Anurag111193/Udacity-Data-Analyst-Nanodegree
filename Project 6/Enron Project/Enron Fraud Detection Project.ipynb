{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enron Fraud Detectors using Enron Emails and Financial Data.\n",
    "\n",
    "\n",
    "### Short Questions\n",
    "Question 1: Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?\n",
    "\n",
    "The goal of this project is to use  financial and email data from [Enron corpus](https://www.cs.cmu.edu/~./enron/) - publicly made by US Federal Energy Regulatory Comission during its investgiation of Enron, which comprised email and financial data of 146 public (files), the majority of which are of management of Enron, to generate a predictive design that could spot an individual as a \"Person of Interest (POI). The corpus is actually widely useful for various device studying problem and though it was described aready, the worthiness could be the potential software for close cases various other organizations or junk e-mail application that is filtering. The dataset contained 146 files with 1 described ability (POI), 14 features that are financial 6 email element. Within these record, 18 are defined as a \"Person Of  Interest\" (POI). \n",
    "\n",
    "However, the dataset contains numerous people missing value for each feature which can be described as table below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis #\n",
    "Total number of data points: 146\n",
    "Number of Persons of Interest: 18\n",
    "Number of people without Person of Interest label: 128\n",
    "Each person has 21 features available\n",
    "Number of Missing Values for Each Feature:\n",
    "salary: 51\n",
    "to_messages: 60\n",
    "deferral_payments: 107\n",
    "total_payments: 21\n",
    "exercised_stock_options: 44\n",
    "bonus: 64\n",
    "restricted_stock: 36\n",
    "shared_receipt_with_poi: 60\n",
    "restricted_stock_deferred: 128\n",
    "total_stock_value: 20\n",
    "expenses: 51\n",
    "loan_advances: 142\n",
    "from_messages: 60\n",
    "other: 53\n",
    "from_this_person_to_poi: 60\n",
    "poi: 0\n",
    "director_fees: 129\n",
    "deferred_income: 97\n",
    "long_term_incentive: 80\n",
    "email_address: 35\n",
    "from_poi_to_this_person: 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the greatest corporate fraud in which the suspectes are Jefferey Skilling, Key Lay, and Fastow all have dumped large amounts of stock options, and they are all deemed guilty.\n",
    "\n",
    "Here scikit-learn & various machine learning techniques to predict \"Person of Interest\", detecting person using both financial & email-data. Through exploratory data analysis there are 3 records that needs to be removed:\n",
    "\n",
    "- TOTAL: Through visualising using scatter-plot matrix. We found TOTAL are the extreme outlier since it comprised every financial data in it.\n",
    "- THE TRAVEL AGENCY IN THE PARK: This must be a data-entry error that it didn't represent an individual.\n",
    "- LOCKHART EUGENE E: This record contained only NaN data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2: What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it.\n",
    "\n",
    "\n",
    "To select the features scikit-learn SelectKBest is used, the best 10 influential features are extrated and used those featuers for all the algorithms used in the following section. Surprisingly, 9 out of 10 features related to financial data and only 1 features called shared_receipt_with_poi (messages from/to the POI divided by to/from messages from the person) was created. Main purpose of composing ratio of POI message is to expect POI contact each other more often than non-POI and the relationship could be non-linear. \n",
    "\n",
    "The initial assumption behind these features is: the relationship between POI is much more stronger than between POI and non-POIs, and if we quickly did back-of-the-envelope Excel scatter plot, there might be truth to that hypothesis. The fact that shared_receipt_with_poi is included after using SelectKBest proved that this is a crucial features, as they also slightly increased the precision and recall of most of the machine learning algorithms used in later part of the analysis.\n",
    "\n",
    "After feature engineering & using SelectKBest,scaling of all features is done using min-max scalers. As briefly investigated through exporting CSV, we can see all email and financial data are varied by several order of magnitudes. Therefore, it is vital that we feature-scaling for the features to be considered evenly. For a comprehensive look on the chosen features, we can look at their respective score after using SelectKBest by the table below:\n",
    "\n",
    "| Selected Features       | Score↑ |\n",
    "| :---------------------- | -----: |\n",
    "| exercised_stock_options | 22.510 |\n",
    "| total_stock_value       | 22.349 |\n",
    "| bonus                   | 20.792 |\n",
    "| salary                  | 18.289 |\n",
    "| deferred_income         | 11.425 |\n",
    "| long_term_incentive     |  9.922 |\n",
    "| restricted_stock        |  9.284 |\n",
    "| total_payments          |  8.772 |\n",
    "| shared_receipt_with_poi |  8.589 |\n",
    "| loan_advances           |  7.184 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3: *What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?*__\n",
    "\n",
    "After learning more than 10 algorithm the Random Forest Classifer, Support Vector Machine & Logistic Regression have the potential to be improved further. Without any tuning, K-means clustering performed reasonably sufficient with precision & recall rate both larger than 0.15. Logistic regression is using widely in medical & law field, most prominent case is to predict tumor benign/malignancy or guilty/no-guilty law case and I would love to test, and recently with e-mail spamming classifer. Although initially, the result was not as expected, I believe with further tuning we can come up with a much better result. \n",
    "\n",
    "Post-tuning result is summarized as tabel below:\n",
    "\n",
    "| Algorithm               | Precision | Recall |\n",
    "| :---------------------- |--------|  -----: |\n",
    "| Logistic Regression     | 0.3703  | 0.4203 |\n",
    "| K Means     | 0.3858  | 0.3573 |\n",
    "| Support Vector Classifier | 0.5030  | 0.2174 |\n",
    "| Random Forest Classifier  | 0.3299  | 0.1612 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4: What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm?\n",
    "\n",
    "Parameters tuning refers to the adjustment of the algorithm when training, in order to improve the fit on the test set. Parameter can influence the outcome of the learning process, the more tuned the parameters, the more biased the algorithm will be to the training data & test harness. The strategy can be effective but it can also lead to more fragile models & overfit the test harness but don't perform well in practice\n",
    "\n",
    "With every algorithm implemented, I tune as much as I could with only marginal success & unremmarkable improvement but there was an exception with significant success with Logistic Regression & K-Mean Clustering. Manually searching through the documentation, I came up with these following paremeters:\n",
    "\n",
    "Logistic regression: C (inverse regularization), class weight (weights associated with classes), max iteration (maximum number of iterations taken for the solvers to converge), random_state (the seed of the pseudo random number generator to use when shuffling the data), solver (using 'liblinear' since we have very small dataset).\n",
    "\n",
    "```\n",
    "C=1e-08, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, \n",
    "max_iter=100, multi_class='ovr', penalty='l2', random_state=42, solver='liblinear', tol=0.001, verbose=0))\n",
    "\n",
    "```\n",
    "K-means clustering: \n",
    "\n",
    "```\n",
    "KMeans(copy_x=True, init='k-means++', max_iter=300, n_clusters=2, n_init=10,\n",
    "       n_jobs=1, precompute_distances='auto', random_state=None, tol=0.001,\n",
    "      verbose=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5: What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?\n",
    "\n",
    "Validation comprises set of techniques to make sure our model generalizes with the remaining part of the dataset. The mistake done is over-fitting where the model performed well on training set but have substantial lower result on test set. In order to overcome such mistake, we can conduct cross-validation (provided by the evaluate function in poi_id.py where I start 1000 trials and divided the dataset into 3:1 training-to-test ratio. Main reason why we would use StratifiedSuffleSplit rather than other splitting techniques avaible is due to the nature of our dataset, which is extremely small with only 14 Persons of Interest. A single split into a training & test set would not give a better estimate of error accuracy. Therefore, we need to randomly split the data into multiple trials while keeping the fraction of POIs in each trials relatively constant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 6: Give at least 2 evaluation metrics, and your average performance for each of them. Explain an interpretation of your metrics that says something human-understandable about your algorithm's performance\n",
    "\n",
    "I used precision & recall as 2 main evaluation metrics. The best performance belongs to logistic regression which is also the final model of choice, as logistic regression is also widely used in text classification, we can actually extend this model for email classification if needed. Precision refer to the ratio of true positive (predicted as POI) to the records that are actually POI while recall described ratio of true positives to people flagged as POI. Essentially speaking, with a precision score of 0.386, it tells us if this model predicts 100 POIs, there would be 38 people are actually POIs and the rest 62 are innocent. With recall score of 0.4252, this model finds 42% of all real POIs in prediction. This model is amazingly perfect for finding bad guys without missing anyone, but with 42% probability of wrong\n",
    "\n",
    "With a precision score of 0.38, it tells us that if this model predicts 100 POIs, then the chance would be 38 people who are truely POIs and the rest 62 are innocent. On the other hand, with a recall score of 0.415, this model can find 42% of all real POIs in prediction. Due to the nature of the dataset, accuracy is not a good measurement as even if non-POI are all flagged, the accuracy score will yield that the model is a success.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "- [Introduction to Machine Learning (Udacity)](https://www.udacity.com/course/viewer#!/c-ud120-nd)\n",
    "- [MITx Analytics Edge](https://www.edx.org/course/analytics-edge-mitx-15-071x-0)\n",
    "- [scikit-learn Documentation](http://scikit-learn.org/stable/documentation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files\n",
    "- `data/`: dataset files and `pickle` objects\n",
    "- `tools/`: helper tools and functions\n",
    "- `scripts/poi_id.py`: main submission file - POI identifier\n",
    "- `scripts/tester.py`: Udacity-provided file, produce test result for submission\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
