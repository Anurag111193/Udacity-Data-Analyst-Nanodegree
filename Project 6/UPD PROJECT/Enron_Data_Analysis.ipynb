{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TO IDENTIFY ENRON FRAUD FROM EMAIL LIST PROVIDED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "By Anurag Sukhija"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In 2002 there was fraud at the largest corporate company in US at that time which was ENRON. We are given with the email list of the persons from which we have to find persons of interest and have to apply machine learning to predict the results of the involvement of the POI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis is started by finding the POI: \n",
    "    Here is the List of emails of all the Sorted POI:\n",
    "        kevin.hannon@enron.com\n",
    "wes.colwell@enron.com\n",
    "paula.rieker@enron.com\n",
    "michael.kopper@enron.com\n",
    "rex.shelby@enron.com\n",
    "david.delainey@enron.com\n",
    "kenneth.lay@enron.com\n",
    "raymond.bowen@enron.com\n",
    "tim.belden@enron.com\n",
    "andrew.fastow@enron.com\n",
    "christopher.calger@enron.com\n",
    "ken.rice@enron.com\n",
    "jeff.skilling@enron.com\n",
    "scott.yeager@enron.com\n",
    "joe.hirko@enron.com\n",
    "mark.koenig@enron.com\n",
    "richard.causey@enron.com\n",
    "ben.glisan@enron.com\n",
    "\n",
    "There are 18 POIs in total.\n",
    "There are 146 Users data in total.\n",
    "and finally 128 Non POI.\n",
    "\n",
    "The dataset contains two types of attributes one which are related to mails like emal, from email, to email and the others are related to money like salary, stocks, expenses, lonas etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the data is analyzed we can find the below data values:\n",
    "    {'salary': 274975, 'to_messages': 873, 'deferral_payments': 'NaN', 'total_payments': 1272284, 'exercised_stock_options': 384728, 'bonus': 600000, 'restricted_stock': 393818, 'shared_receipt_with_poi': 874, 'restricted_stock_deferred': 'NaN', 'total_stock_value': 778546, 'expenses': 125978, 'loan_advances': 'NaN', 'from_messages': 16, 'other': 200308, 'from_this_person_to_poi': 6, 'poi': True, 'director_fees': 'NaN', 'deferred_income': 'NaN', 'long_term_incentive': 71023, 'email_address': 'ben.glisan@enron.com', 'from_poi_to_this_person': 52}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "we can observe that features like total_payments, salary are variying which are causing outliers the keys creating the most outliers are:\n",
    "     TOTAL and THE TRAVEL AGENCY IN THE PARK.\n",
    "     \n",
    "     \n",
    "The outlier was removed using:\n",
    "outliers = ['TOTAL', 'THE TRAVEL AGENCY IN THE PARK']\n",
    "for outlier in outliers:\n",
    "    data_dict.pop(outlier, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Variable Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes I have created Some varibales, First is email_exist which will tell that the poi are having a valid email if not they can be discarded. Second is the Shared_poi which will tell that if the POI's have done communication between them. Next is the ratio of the mails sent from poi and last is the emails received by the poi. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have also used SelectK best features to find 9 best features we can use which are:\n",
    "9 selected features: ['salary', 'bonus', 'total_stock_value', 'shared_poi_per_email', 'fraction_to_poi', 'exercised_stock_options', 'deferred_income', 'restricted_stock', 'long_term_incentive']\n",
    "\n",
    "\n",
    "\n",
    "Since using selecktKbest we are removing low correlation value atttibutes the corelation of the attributes are as follows:\n",
    "\n",
    "[ 18.28968404   0.22461127   8.77277773   7.18405566  20.79225205\n",
    "   0.06549965  11.45847658  24.18289868   6.09417331  24.81507973\n",
    "   4.18747751   9.92218601   9.21281062   2.1263278    1.64634113\n",
    "   6.10692533   3.12809175  16.40971255   9.10126874   0.16970095\n",
    "   8.58942073   1.64634113   5.24344971   0.16970095   2.38261211\n",
    "   8.58942073  16.40971255]\n",
    "   \n",
    "   \n",
    "\tvariable\tscore\n",
    "1\texercised_stock_options\t24.72\n",
    "2\ttotal_stock_value\t24.4\n",
    "3\tfraction_to_poi\t22.98\n",
    "4\tbonus\t22.07\n",
    "5\tsalary\t16.54\n",
    "6\trestricted_stock\t12.12\n",
    "7\tlong_term_incentive\t11.32\n",
    "8\tdeferred_income\t11.26\n",
    "9\texpenses\t9.41\n",
    "10\ttotal_payments\t9.24\n",
    "11\tloan_advances\t7.49\n",
    "12\tshared_receipt_with_poi\t7.2\n",
    "13\tshared_poi_per_email\t6.96\n",
    "14\temail_exists\t5.35\n",
    "15\tother\t4.62\n",
    "16\tdirector_fees\t2.02\n",
    "17\tfraction_from_poi\t1.84\n",
    "18\tto_messages\t1.35\n",
    "19\tfrom_messages\t0.15\n",
    "20\tdeferral_payments\t0.09\n",
    "\n",
    "Then using the SelectKBest function in sklearn.feature_selection module, I got the highest 20 features and their scores. ‘fraction_to_poi’ ranks the 3th, ‘shared_poi_per_email’, ‘email_exists’ and ‘fraction_from_poi’ in the 13th, 14th and 17th place, respectively.\n",
    "\n",
    "As observed from the chart below the Score of the variables is continuously decreasing we have scores high as 25 and low as 1 therefore we are using SelectKBest features to fetch the best features. SelectKBest are used since it finds the best features according to the dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/score.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: Techniques Used: \n",
    "        The following Techniques are used for analysis:\n",
    "            1. Naive Bayes\n",
    "            2. Logistic Regression\n",
    "            3. SVC\n",
    "            4. Random Forest\n",
    "            5. K means\n",
    "            6. Ada Boost\n",
    "            7. Gradient Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Naive Bayes:\n",
    "    \n",
    "    In this technique I have used 8 best features and Standard Scaling as well as pipelining is applied: \n",
    "        clf = Pipeline(steps=[\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', GaussianNB())\n",
    "    ])\n",
    "\n",
    "        the result Obtained by the classifier is: \n",
    "            \n",
    "            \tAccuracy: 0.84650\tPrecision: 0.45370\tRecall: 0.36500\tF1: 0.40454\tF2: 0.37985\n",
    "\tTotal predictions: 14000\tTrue positives:  730\tFalse positives:  879\tFalse negatives: 1270\tTrue negatives: 11121"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. K Means: \n",
    "    \n",
    "    In this technique too Standard Scaling and pipelining is used:\n",
    "         clf = Pipeline(steps=[\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', KMeans(n_clusters=2, tol=0.001))\n",
    "    ])\n",
    "            \n",
    "            the result obtained is:\n",
    "                \n",
    "                \tAccuracy: 0.63707\tPrecision: 0.16778\tRecall: 0.38900\tF1: 0.23444\tF2: 0.30783\n",
    "\tTotal predictions: 14000\tTrue positives:  778\tFalse positives: 3859\tFalse negatives: 1222\tTrue negatives: 8141"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Ada Boost Algorithm: \n",
    "    In this technique too Standard Scaling and pipelining is used, also the technique is implemented using Decision Tree: \n",
    "        dt = DecisionTreeClassifier() \n",
    "    clf = Pipeline(steps=[\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', AdaBoostClassifier(n_estimators=100, base_estimator=dt,learning_rate=1))\n",
    "    ])\n",
    "    \n",
    "    the result Obtained by the technique is:\n",
    "        \tAccuracy: 0.79286\tPrecision: 0.26514\tRecall: 0.25400\tF1: 0.25945\tF2: 0.25615\n",
    "\tTotal predictions: 14000\tTrue positives:  508\tFalse positives: 1408\tFalse negatives: 1492\tTrue negatives: 10592\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Gradient Boost Algorithm:\n",
    "    In this technique too Standard Scaling and pipelining is used:\n",
    "        \n",
    "        clf = Pipeline(steps=[\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1))\n",
    "    ])\n",
    "\n",
    "        \n",
    "        the result obtained by the technique is: \n",
    "            \tAccuracy: 0.79893\tPrecision: 0.25701\tRecall: 0.21550\tF1: 0.23443\tF2: 0.22269\n",
    "\tTotal predictions: 14000\tTrue positives:  431\tFalse positives: 1246\tFalse negatives: 1569\tTrue negatives: 10754"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Random Forest Algorithm:\n",
    "    In this technique too Standard Scaling and pipelining is used:\n",
    "        clf = Pipeline(steps=[\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', RandomForestClassifier(max_depth = 5, \n",
    "                                 max_features = 'sqrt', \n",
    "                                 n_estimators = 10, \n",
    "                                 random_state = 42))\n",
    "\n",
    "    ])\n",
    "        \n",
    "        the result obtained by the technique is:\n",
    "            \n",
    "            \tAccuracy: 0.85787\tPrecision: 0.41990\tRecall: 0.17300\tF1: 0.24504\tF2: 0.19606\n",
    "\tTotal predictions: 15000\tTrue positives:  346\tFalse positives:  478\tFalse negatives: 1654\tTrue negatives: 12522\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Logistic Regression: \n",
    "    In this technique too Standard Scaling and pipelining is used:\n",
    "        clf = Pipeline(steps=[\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', LogisticRegression(tol = 0.001, C = 10**-8, penalty = 'l2', \n",
    "                                              random_state = 42, class_weight = 'auto'))\n",
    "    ])\n",
    "        \n",
    "         the result obtained by the technique is:\n",
    "                \n",
    "                \tAccuracy: 0.79200\tPrecision: 0.32968\tRecall: 0.54200\tF1: 0.40998\tF2: 0.48016\n",
    "\tTotal predictions: 15000\tTrue positives: 1084\tFalse positives: 2204\tFalse negatives:  916\tTrue negatives: 10796\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Support Vector Machine:\n",
    "    \n",
    "    In this technique too Standard Scaling and pipelining is used:\n",
    "        clf = Pipeline(steps=[\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', SVC(kernel = 'rbf', C = 1000, gamma = 0.0001, \n",
    "                               random_state = 42, class_weight = 'auto'))\n",
    "    ])\n",
    "        \n",
    "        the result obtained by the technique is:\n",
    "            \n",
    "            \tAccuracy: 0.75143\tPrecision: 0.32596\tRecall: 0.69300\tF1: 0.44338\tF2: 0.56562\n",
    "\tTotal predictions: 14000\tTrue positives: 1386\tFalse positives: 2866\tFalse negatives:  614\tTrue negatives: 9134"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Final output table is shown as follows:\n",
    "\n",
    "Classifier\tAccuracy\tPrecision\tRecall\tF1\tF2\n",
    "GaussianNB\t0.846\t0.454\t0.365\t0.405\t0.380\n",
    "RandomForestClassifier\t0.857\t0.419\t0.173\t0.259\t0.256\n",
    "LogisticRegression\t0.792\t0.330\t0.542\t0.410\t0.480\n",
    "SVC\t0.751\t0.326\t0.693\t0.443\t0.566\n",
    "Gradient Boost\t0.798\t0.257\t0.215\t0.234\t0.222\n",
    "Ada Boost\t0.792\t0.265\t0.254\t0.259\t0.256\n",
    "K Means \t0.637\t0.167\t0.389\t0.234\t0.307"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as observed from the table one of the algorithm good accuracy, precision and recall is the Logistic regression. Since I have optimized the algorithms which has lead to better results but if algorithms are not applied a minor effect can be seen on the other algorithms but Logistic Shows a great difference:\n",
    "    \n",
    "    For example idf normal LR is applied the results are:\n",
    "       Accuracy :0.850\n",
    "       Precision 0.367\n",
    "       Recall: 0.170 \n",
    "       F1: 0.233\n",
    "       F2: 0.191\n",
    "        \n",
    "        We can observe that even though Accuracy is more the Precision and recall parametrs are badly affected. \n",
    "        \n",
    "        \n",
    "    Similarly if LR is tuned with random state and class weight the results are:\n",
    "        \n",
    "       Accuracy : 0.666\n",
    "       Precision 0.274\n",
    "       Recall: 0.611 \n",
    "       F1: 0.421\n",
    "       F2: 0.621\n",
    "        \n",
    "        \n",
    "    And we have selected only random state giving one of the perfect results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4: What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well? How did you tune the parameters of your particular algorithm?\n",
    "    \n",
    "    \n",
    "    Parameters of any algorithm can change its behaviour towards the test data as it provides a better fit. the more is the algorithm tuned the more biased it is towards the data.\n",
    "    \n",
    "    In this data 8 algorithms have been applied and all of them are tried for tuning. apart from Logistic Regression others shows no or minimum change wrt to the data.\n",
    "    \n",
    "    The parameters of LR is shown below: \n",
    "   Logistic regression: C (inverse regularization), class weight (weights associated with classes), max iteration (maximum number of iterations taken for the solvers to converge), random_state (the seed of the pseudo random number generator to use when shuffling the data), solver (using 'liblinear' since we have very small dataset).\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', LogisticRegression(tol = 0.001, C = 10**-8, penalty = 'l2', \n",
    "                                          random_state = 42))\n",
    "])\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', LogisticRegression(tol = 0.001, C = 10**-8, penalty = 'l2', \n",
    "                                          random_state = 42, class_weight = 'auto'))\n",
    "])\n",
    "\n",
    "\n",
    "the outputs of the tuning are as follows:\n",
    "\n",
    "LogisticRegression\tAccuracy\tPrecision\tRecall\tF1\tF2\n",
    "Not tuned \t0.85\t0.367\t0.17\t0.232\t0.190\n",
    "Tuned  [FINAL]\t0.792\t0.329\t0.542\t0.406\t0.48\n",
    "Tuned w/ auto class_weight \t0.665\t0.274\t0.911\t0.421\t0.621\n",
    "\n",
    "Since the second tuned model gives the best accuracy precision and recall values that is it provides the best precision and recall values therefore it is selected and also LR is selected because of its extensibity in text mining applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5: What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?\n",
    "    \n",
    "  since the data overfits some or other validation technique is required to make sure that the system provides generalize result for rest of the database. to cross verify i have used:\n",
    "  \n",
    "  data = featureFormat(my_dataset, my_feature_list)\n",
    "  labels, features = targetFeatureSplit(data)\n",
    "  \n",
    "  The above code will use StratifiedSuffleSplit, it used because the dataset used has a less number of POI and rest of the data is also small making it neccasary to perform a random split of data to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 6: Give at least 2 evaluation metrics, and your average performance for each of them. Explain an interpretation of your metrics that says something human-understandable about your algorithm's performance\n",
    "    \n",
    "    Here in this analysis the evaluation metrices are precision and recall. Precision is the value which depicts the true positive values of actual POI whereas recall defines the true positive value of people marked a flag POI.\n",
    "    \n",
    "    Here we are using LR as the final model with Precision as 0.32 and 0.54 as precision and recall respectivels. Since we have to search for POI from the data that is we are doing Recall here the recall percentage is 54% that is if 100 people are searched the recall will be 54 poi now to enhance this precision is applied, it shows that what is the percentage of correctness of the recall values here the precision is 32% that is from 54 recalled POI 17 people are actual culprit rest canbe innocent. Due to such small and unclear data the system has a low pecision and recall values. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum up, we started the analysis with studying the dataset and removing the outliers Total and The trvel agency in the park. Which were not making much sense while analyzing the data as a whole. to help us understand better I created a few new variables like shared poi and email to make the analysis easy and all the heads are tested using the SelectKBest from which 9 best features are extracted, In which we can observe that shared poi is present as one of the best keys for analysis. Then I applied multiple Machine Leearning classifiers to test the accuracy and its scoring based on precision, recall, F1 and F2. Since the pipelining, Scalar and grid search is used we can observe that only Logistic Regression is an algorithm which is giving drastic changes while tuning the algorithm therefore I selected LR as the final Method, also it is one of the leading teechniques for text mining and if features are also added to the system the LR can easily adapt to it and can provide good results. The system can easily be extended if there is no lossy data or more confined heads like investments in the past month or so which can give us much more better results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#References:\n",
    "\n",
    "[1]http://scikit-learn.org/stable/auto_examples/grid_search_text_feature_extraction.html\n",
    "\n",
    "[2]http://discussions.udacity.com/t/when-to-use-feature-scaling/12923\n",
    "\n",
    "[3]http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "    \n",
    "[4]https://www.analyticsvidhya.com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning/\n",
    "\n",
    "[5] https://www.quora.com/What-is-the-best-way-to-understand-the-terms-precision-and-recall\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
