{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TO IDENTIFY ENRON FRAUD FROM EMAIL LIST PROVIDED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "By Anurag Sukhija"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In 2002 there was fraud at the largest corporate company in US at that time which was ENRON. We are given with the email list of the persons from which we have to find persons of interest and have to apply machine learning to predict the results of the involvement of the POI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis is started by finding the POI: \n",
    "    Here is the List of emails of all the Sorted POI:\n",
    "        kevin.hannon@enron.com\n",
    "wes.colwell@enron.com\n",
    "paula.rieker@enron.com\n",
    "michael.kopper@enron.com\n",
    "rex.shelby@enron.com\n",
    "david.delainey@enron.com\n",
    "kenneth.lay@enron.com\n",
    "raymond.bowen@enron.com\n",
    "tim.belden@enron.com\n",
    "andrew.fastow@enron.com\n",
    "christopher.calger@enron.com\n",
    "ken.rice@enron.com\n",
    "jeff.skilling@enron.com\n",
    "scott.yeager@enron.com\n",
    "joe.hirko@enron.com\n",
    "mark.koenig@enron.com\n",
    "richard.causey@enron.com\n",
    "ben.glisan@enron.com\n",
    "There are 18 POIs in total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the data is analyzed we can find the below data values:\n",
    "    {'salary': 274975, 'to_messages': 873, 'deferral_payments': 'NaN', 'total_payments': 1272284, 'exercised_stock_options': 384728, 'bonus': 600000, 'restricted_stock': 393818, 'shared_receipt_with_poi': 874, 'restricted_stock_deferred': 'NaN', 'total_stock_value': 778546, 'expenses': 125978, 'loan_advances': 'NaN', 'from_messages': 16, 'other': 200308, 'from_this_person_to_poi': 6, 'poi': True, 'director_fees': 'NaN', 'deferred_income': 'NaN', 'long_term_incentive': 71023, 'email_address': 'ben.glisan@enron.com', 'from_poi_to_this_person': 52}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "we can observe that features like total_payments, salary are variying which are causing outliers the keys creating the most outliers are:\n",
    "     TOTAL and THE TRAVEL AGENCY IN THE PARK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Variable Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes I have created Some varibales, First is email_exist which will tell that the poi are having a valid email if not they can be discarded. Second is the Shared_poi which will tell that if the POI's have done communication between them. Next is the ration of the mails sent from poi and last is the emails received by the poi. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have also used SelectK best features to find 9 best features we can use which are:\n",
    "9 selected features: ['salary', 'bonus', 'total_stock_value', 'shared_poi_per_email', 'fraction_to_poi', 'exercised_stock_options', 'deferred_income', 'restricted_stock', 'long_term_incentive']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: Techniques Used: \n",
    "        The following Techniques are used for analysis:\n",
    "            1. Naive Bayes\n",
    "            2. Logistic Regression\n",
    "            3. SVC\n",
    "            4. Random Forest\n",
    "            5. K means\n",
    "            6. Ada Boost\n",
    "            7. Gradient Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Naive Bayes:\n",
    "    \n",
    "    In this technique I have used 8 best features and Standard Scaling as well as pipelining is applied: \n",
    "        clf = Pipeline(steps=[\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', GaussianNB())\n",
    "    ])\n",
    "\n",
    "        the result Obtained by the classifier is: \n",
    "            \n",
    "            \tAccuracy: 0.84650\tPrecision: 0.45370\tRecall: 0.36500\tF1: 0.40454\tF2: 0.37985\n",
    "\tTotal predictions: 14000\tTrue positives:  730\tFalse positives:  879\tFalse negatives: 1270\tTrue negatives: 11121"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. K Means: \n",
    "    \n",
    "    In this technique too Standard Scaling and pipelining is used:\n",
    "         clf = Pipeline(steps=[\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', KMeans(n_clusters=2, tol=0.001))\n",
    "    ])\n",
    "            \n",
    "            the result obtained is:\n",
    "                \n",
    "                \tAccuracy: 0.63707\tPrecision: 0.16778\tRecall: 0.38900\tF1: 0.23444\tF2: 0.30783\n",
    "\tTotal predictions: 14000\tTrue positives:  778\tFalse positives: 3859\tFalse negatives: 1222\tTrue negatives: 8141"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Ada Boost Algorithm: \n",
    "    In this technique too Standard Scaling and pipelining is used, also the technique is implemented using Decision Tree: \n",
    "        dt = DecisionTreeClassifier() \n",
    "    clf = Pipeline(steps=[\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', AdaBoostClassifier(n_estimators=100, base_estimator=dt,learning_rate=1))\n",
    "    ])\n",
    "    \n",
    "    the result Obtained by the technique is:\n",
    "        \tAccuracy: 0.79286\tPrecision: 0.26514\tRecall: 0.25400\tF1: 0.25945\tF2: 0.25615\n",
    "\tTotal predictions: 14000\tTrue positives:  508\tFalse positives: 1408\tFalse negatives: 1492\tTrue negatives: 10592\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Gradient Boost Algorithm:\n",
    "    In this technique too Standard Scaling and pipelining is used:\n",
    "        \n",
    "        clf = Pipeline(steps=[\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1))\n",
    "    ])\n",
    "\n",
    "        \n",
    "        the result obtained by the technique is: \n",
    "            \tAccuracy: 0.79893\tPrecision: 0.25701\tRecall: 0.21550\tF1: 0.23443\tF2: 0.22269\n",
    "\tTotal predictions: 14000\tTrue positives:  431\tFalse positives: 1246\tFalse negatives: 1569\tTrue negatives: 10754"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Random Forest Algorithm:\n",
    "    In this technique too Standard Scaling and pipelining is used:\n",
    "        clf = Pipeline(steps=[\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', RandomForestClassifier(max_depth = 5, \n",
    "                                 max_features = 'sqrt', \n",
    "                                 n_estimators = 10, \n",
    "                                 random_state = 42))\n",
    "\n",
    "    ])\n",
    "        \n",
    "        the result obtained by the technique is:\n",
    "            \n",
    "            \tAccuracy: 0.85787\tPrecision: 0.41990\tRecall: 0.17300\tF1: 0.24504\tF2: 0.19606\n",
    "\tTotal predictions: 15000\tTrue positives:  346\tFalse positives:  478\tFalse negatives: 1654\tTrue negatives: 12522\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Logistic Regression: \n",
    "    In this technique too Standard Scaling and pipelining is used:\n",
    "        clf = Pipeline(steps=[\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', LogisticRegression(tol = 0.001, C = 10**-8, penalty = 'l2', \n",
    "                                              random_state = 42, class_weight = 'auto'))\n",
    "    ])\n",
    "        \n",
    "         the result obtained by the technique is:\n",
    "                \n",
    "                \tAccuracy: 0.79200\tPrecision: 0.32968\tRecall: 0.54200\tF1: 0.40998\tF2: 0.48016\n",
    "\tTotal predictions: 15000\tTrue positives: 1084\tFalse positives: 2204\tFalse negatives:  916\tTrue negatives: 10796\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Support Vector Machine:\n",
    "    \n",
    "    In this technique too Standard Scaling and pipelining is used:\n",
    "        clf = Pipeline(steps=[\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', SVC(kernel = 'rbf', C = 1000, gamma = 0.0001, \n",
    "                               random_state = 42, class_weight = 'auto'))\n",
    "    ])\n",
    "        \n",
    "        the result obtained by the technique is:\n",
    "            \n",
    "            \tAccuracy: 0.75143\tPrecision: 0.32596\tRecall: 0.69300\tF1: 0.44338\tF2: 0.56562\n",
    "\tTotal predictions: 14000\tTrue positives: 1386\tFalse positives: 2866\tFalse negatives:  614\tTrue negatives: 9134"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Final output table is shown as follows:\n",
    "\n",
    "Classifier\tAccuracy\tPrecision\tRecall\tF1\tF2\n",
    "GaussianNB\t0.846\t0.454\t0.365\t0.405\t0.380\n",
    "RandomForestClassifier\t0.857\t0.419\t0.173\t0.259\t0.256\n",
    "LogisticRegression\t0.792\t0.330\t0.542\t0.410\t0.480\n",
    "SVC\t0.751\t0.326\t0.693\t0.443\t0.566\n",
    "Gradient Boost\t0.798\t0.257\t0.215\t0.234\t0.222\n",
    "Ada Boost\t0.792\t0.265\t0.254\t0.259\t0.256\n",
    "K Means \t0.637\t0.167\t0.389\t0.234\t0.307"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as observed from the table one of the algorithm good accuracy, precision and recall is the Logistic regression. Since I have optimized the algorithms which has lead to better results but if algorithms are not applied a minor effect can be seen on the other algorithms but Logistic Shows a great difference:\n",
    "    \n",
    "    For example idf normal LR is applied the results are:\n",
    "       Accuracy :0.850\n",
    "       Precision 0.367\n",
    "       Recall: 0.170 \n",
    "       F1: 0.233\n",
    "       F2: 0.191\n",
    "        \n",
    "        We can observe that even though Accuracy is more the Precision and recall parametrs are badly affected. \n",
    "        \n",
    "        \n",
    "    Similarly if LR is tuned with random state and class weight the results are:\n",
    "        \n",
    "       Accuracy : 0.666\n",
    "       Precision 0.274\n",
    "       Recall: 0.611 \n",
    "       F1: 0.421\n",
    "       F2: 0.621\n",
    "        \n",
    "        \n",
    "    And we have selected only random state giving one of the perfect results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum up, we started the analysis with studying the dataset and removing the outliers Total and The trvel agency in the park. Which were not making much sense while analyzing the data as a whole. to help us understand better I created a few new variables like shared poi and email to make the analysis easy and all the heads are tested using the SelectKBest from which 9 best features are extracted, In which we can observe that shared poi is present as one of the best keys for analysis. Then I applied multiple Machine Leearning classifiers to test the accuracy and its scoring based on precision, recall, F1 and F2. Since the pipelining, Scalar and grid search is used we can observe that only Logistic Regression is an algorithm which is giving drastic changes while tuning the algorithm therefore I selected LR as the final Method, also it is one of the leading teechniques for text mining and if features are also added to the system the LR can easily adapt to it and can provide good results. The system can easily be extended if there is no lossy data or more confined heads like investments in the past month or so which can give us much more better results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#References:\n",
    "\n",
    "[1]http://scikit-learn.org/stable/auto_examples/grid_search_text_feature_extraction.html\n",
    "\n",
    "[2]http://discussions.udacity.com/t/when-to-use-feature-scaling/12923\n",
    "\n",
    "[3]http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "    \n",
    "[4]https://www.analyticsvidhya.com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning/\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
